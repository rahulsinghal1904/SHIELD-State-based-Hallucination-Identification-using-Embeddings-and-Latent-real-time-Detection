# SHIELD : State-based Hallucination Identification using Embeddings and Latent real-time Detection

Large language models (LLMs) excel in NLP tasks but often generate hallucinationsâ€”factually incorrect outputs that limit their use in fields like healthcare and law. Traditional detection methods require labeled data and resource-heavy post-processing, impacting scalability. We present SHIELD (State-based Hallucination Identification using Embeddings and Latent Detection), an unsupervised, real-time framework that detects hallucinations by analyzing LLM's internal states. This approach eliminates the need for manual labeling and reduces computational load, achieving strong results: Sentence-Level AUCs of 0.7329 on Falcon and 0.8805 on GPT-J, outperforming traditional baselines. SHIELD offers a scalable solution, boosting LLM safety and reliability in critical applications.